{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67f3d7b4-cdad-4fcd-b9e5-293e22107804",
   "metadata": {},
   "source": [
    "# Temperature, Max Tokens, and Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f22e56c",
   "metadata": {},
   "source": [
    "Welcome back!\n",
    "\n",
    "In this section, weâ€™ve discussed the roles we can employ when passing a message to a model: a system, user, or assistant role. \n",
    "We then used a system message to create a chatbot that adopts a sarcastic persona.\n",
    "\n",
    "In this lesson, weâ€™ll discuss a few more parameters that can affect the modelâ€™s response:\n",
    "<ul>\n",
    "  <li>Its maximum number of completion tokens,</li>\n",
    "  <li>Level of randomness, and </li>\n",
    "  <li>The option to stream it.</li>\n",
    "</ul>\n",
    "\n",
    "Letâ€™s begin with the tokens. \n",
    "\n",
    "As discussed earlier, OpenAI sets its model prices based on the number of input tokens (the ones we feed to the model) and the number of completion tokens (those the model generates). Both are capped to prevent users from inputting an excessive number of tokens and to stop the models from generating endless text. Still, itâ€™s essential to have additional control over the amount of text generated. Models often tend to be wordy and provide more information than needed. This can pose a problem in the long run since we pay for the tokens the model outputs.\n",
    "\n",
    "As a side note, this pricing only applies when using OpenAIâ€™s models through the API. In contrast, the ChatGPT platform is subscription-based rather than token-based, and you donâ€™t need to worry about the model generating long texts. \n",
    "\n",
    "All right. \n",
    "\n",
    "Itâ€™s time we see the token parameter in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fb9401-edaf-49a9-8bde-958f8ea58b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1260d8b-720e-4ab2-8401-48f7ae60f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29101bb-60f9-4d90-b6ec-ff3433d2eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e07b869-3991-4fdd-b702-a0681023d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63411b61",
   "metadata": {},
   "source": [
    "When defining the **completion** variable, keep the system message the same. However, letâ€™s test the model with a different user message. \n",
    "For example:\n",
    "*Could you explain briefly what a black hole is?*\n",
    "\n",
    "Define the **max_tokens** parameter by assigning a completion tokens limit of 250. Run the cell and print out the content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eef8f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(model = 'gpt-4', \n",
    "                                            messages = [{'role':'system', \n",
    "                                                         'content':''' You are Marv, a chatbot that reluctantly \n",
    "                                                         answers questions with sarcastic responses. '''}, \n",
    "                                                        {'role':'user', \n",
    "                                                         'content':''' Could you explain briefly what a black hole is? '''}], \n",
    "                                            max_tokens = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77489bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822510ec-11de-424d-9c0c-8404f7bd0250",
   "metadata": {},
   "source": [
    "As always, our sarcastic chatbot gives an excellent response. ðŸ˜Š "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b535b9",
   "metadata": {},
   "source": [
    "Letâ€™s now see how it performs when we narrow the cap down to 50 tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955348ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(model = 'gpt-4', \n",
    "                                            messages = [{'role':'system', \n",
    "                                                         'content':''' You are Marv, a chatbot that reluctantly \n",
    "                                                         answers questions with sarcastic responses. '''}, \n",
    "                                                        {'role':'user', \n",
    "                                                         'content':''' Could you explain briefly what a black hole is? '''}], \n",
    "                                            max_tokens = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113be856",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4aa2aa-92ff-41b8-8b42-a2bae711cdc0",
   "metadata": {},
   "source": [
    "This time, we get a much shorter (maybe even incomplete) response. So, we need to be careful not to limit the model too much. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966c49f0",
   "metadata": {},
   "source": [
    "Iâ€™ll now revert to 250 completion tokens.\n",
    "\n",
    "Another parameter weâ€™ll use throughout the course is **temperature**. It accepts values from 0 to 2, where higher values increase response randomness. Although it defaults to 1, let's explore its behavior at the extremes. \n",
    "\n",
    "Set the modelâ€™s temperature to 0 and remove the sarcastic condition to create more of an educative rather than a sarcastic bot.\n",
    "\n",
    "Letâ€™s again ask it to explain what black holes are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53280556",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(model = 'gpt-4', \n",
    "                                            messages = [{'role':'user', \n",
    "                                                         'content':''' Could you explain briefly what a black hole is? '''}], \n",
    "                                            max_tokens = 250, \n",
    "                                            temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dade0758",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c401f-168c-4faf-b9a8-3b01fdd2f5f2",
   "metadata": {},
   "source": [
    "We obtain an informative response. \n",
    "\n",
    "Now, try and generate a new chat completion by re-running the cell defining the **completion** object. Display the variable. \n",
    "\n",
    "We find the new response is similar to the previous one. Of course, the wording is slightly different in places due to the unpredictable nature of these models, but overall, the two generations are quite alike. \n",
    "Models with lower temperatures can be used when creating a chatbot for educational purposes because the answers must be more academic and informative rather than creative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c677a03",
   "metadata": {},
   "source": [
    "Okay, letâ€™s now increase the temperature to maximum and study these results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c686b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(model = 'gpt-4', \n",
    "                                            messages = [{'role':'user', \n",
    "                                                         'content':''' Could you explain briefly what a black hole is? '''}], \n",
    "                                            max_tokens = 250, \n",
    "                                            temperature = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f42709",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134725b3-bc55-4c87-88e6-486939ae4106",
   "metadata": {},
   "source": [
    "Well, we can all agree that this is far from helpful. The bot deviates from the topic very fast, and not long after, it starts generating meaningless text. So, such large temperature values are rarely helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0134b921",
   "metadata": {},
   "source": [
    "Iâ€™ll switch back to a temperature value of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ab93ae-6a39-45a0-8ad1-9b5215804c3f",
   "metadata": {},
   "source": [
    "Another parameter that controls determinism is **seed**â€”like the seeds weâ€™ve fed to machine learning algorithms for reproducibility. \n",
    "When dealing with LLMs, determinism is not guaranteed, but the outcomes will be as similar as possible. (You can experiment with this parameter at home.) From now on, Iâ€™ll set the **temperature** parameter to 0 and **seed** to 365 . I suggest you do the same to obtain results like mine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8cc4c3-0097-432d-8b81-0570a353a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(model = 'gpt-4', \n",
    "                                            messages = [{'role':'user', \n",
    "                                                         'content':''' Could you explain briefly what a black hole is? '''}], \n",
    "                                            max_tokens = 250, \n",
    "                                            temperature = 0, \n",
    "                                            seed = 365)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9df725-0548-4d73-97a6-b882ee80eb7a",
   "metadata": {},
   "source": [
    "Okay, moving on!\n",
    "\n",
    "One way to make a chatbot more responsive and user-friendly is to print out the output continuously rather than displaying it only after itâ€™s fully generated. The **stream** parameter allows us to achieve precisely that. All we need to do is add it to the list of parameters and set its value to **True**.\n",
    "\n",
    "Running the cell below and the following one, we find our variable is no longer a **ChatCompletion** but a **Stream** object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b7322-e123-4a7b-927d-4428b149203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(model = 'gpt-4', \n",
    "                                            messages = [{'role':'user', \n",
    "                                                         'content':''' Could you explain briefly what a black hole is? '''}], \n",
    "                                            max_tokens = 250, \n",
    "                                            temperature = 0, \n",
    "                                            seed = 365, \n",
    "                                            stream = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917b4323-3319-4daf-bd2e-b7e5e423f9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60323557-9e15-4d8a-999c-9226698a43c3",
   "metadata": {},
   "source": [
    "Whatâ€™s important to know about it is that it can be iterated with a simple **for**-loop, like so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad18c8b-486f-4fda-b85e-003f138987ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in completion:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293947d7-f994-439a-9c45-33503f9f25e7",
   "metadata": {},
   "source": [
    "Executing the cell, we find a long list of **ChatCompletionChunk** objects. As their name suggests, they contain only a small chunk of the message.\n",
    "\n",
    "Now, within the **for**-loop, letâ€™s print the content of each chunk. Before executing the **for**-loop, run the cell defining the **completion** variable to generate a new **Stream** object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bccac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in completion:\n",
    "    print(i.choices[0].delta.content, end = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b9b49f-8da6-4bce-8728-2d09a8d2469f",
   "metadata": {},
   "source": [
    "Look at that! \n",
    "Weâ€™ve managed to stream our text on the screen. How cool is that? ðŸ˜Š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac8bb8d-e652-47a7-9fda-cc2f425e9757",
   "metadata": {},
   "source": [
    "This marks the end of our short introduction to the OpenAI API. \n",
    "Iâ€™m convinced you now find creating a chatbot with APIs much more fun and rewarding than merely chatting with ChatGPT. \n",
    "It gives you much more control over the responses and allows you to create some exciting chatbots.\n",
    "\n",
    "But wait until you see what LangChain has to offer. ðŸ˜Š \n",
    "In the following sections, weâ€™ll employ intriguing projects using OpenAIâ€™s models and the LangChain framework. Until then! ðŸ˜Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb591cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "langchain_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
